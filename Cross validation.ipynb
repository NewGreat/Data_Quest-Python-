{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Introduction To Validation(Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        gpa         gre  actual_label\n",
      "0  3.177277  594.102992             0\n",
      "1  3.412655  631.528607             0\n",
      "2  2.728097  553.714399             0\n",
      "3  3.093559  551.089985             0\n",
      "4  3.141923  537.184894             0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "admissions = pd.read_csv(\"admissions.csv\")\n",
    "admissions[\"actual_label\"] = admissions[\"admit\"]\n",
    "admissions = admissions.drop(\"admit\", axis=1)\n",
    "print(admissions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Holdout Validation\n",
    "Instructions Use the NumPy random.permutation function to randomize the index for the admissions Dataframe.\n",
    "Use the loc[] method on the admissions Dataframe to return a new Dataframe in the randomized order. Assign this Dataframe to shuffled_admissions.\n",
    "Select rows 0 to 514 (including row 514) from shuffled_admissions and assign to train.\n",
    "Select the remaining rows and assign to test.\n",
    "Finally, display the first 5 rows in shuffled_admissions.\n",
    "Hint\n",
    "When calling the iloc[] method, recall that the value at the ending index value isn't returned. iloc[0:515] will return rows 0 to 514."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          gpa         gre  actual_label\n",
      "260  3.414036  577.665610             0\n",
      "173  2.932147  564.798764             0\n",
      "256  2.674040  599.895858             0\n",
      "167  2.923581  622.524665             0\n",
      "400  3.414376  704.934217             1\n",
      "415  3.512116  653.744260             1\n",
      "111  3.011533  578.212886             0\n",
      "428  3.552490  667.320643             1\n",
      "312  3.243188  665.619332             0\n",
      "516  4.000000  544.159398             1\n",
      "481  2.975316  601.392062             1\n",
      "264  3.150268  444.223279             0\n",
      "424  3.424274  677.555275             1\n",
      "208  3.733697  524.504412             0\n",
      "308  3.171177  497.847973             0\n",
      "213  3.202666  576.551696             0\n",
      "603  3.077075  713.399147             1\n",
      "467  3.348166  604.252785             1\n",
      "612  3.511204  618.929875             1\n",
      "557  3.725919  641.372775             1\n",
      "502  3.807618  617.107327             1\n",
      "92   2.771824  549.111663             0\n",
      "66   3.027542  595.457519             0\n",
      "219  3.252544  678.615733             0\n",
      "78   3.235723  455.925155             0\n",
      "25   3.443319  629.367403             0\n",
      "430  3.352374  668.034495             1\n",
      "545  3.277949  720.062434             1\n",
      "18   3.212380  545.878970             0\n",
      "358  2.718207  541.374067             0\n",
      "..        ...         ...           ...\n",
      "81   2.829533  538.092052             0\n",
      "494  3.063861  626.018986             1\n",
      "381  3.202438  610.796489             0\n",
      "9    3.910495  463.470183             0\n",
      "322  3.338932  585.244167             0\n",
      "410  3.022044  614.554527             1\n",
      "170  3.090762  609.377330             0\n",
      "563  2.982478  500.380390             1\n",
      "440  3.157379  726.880881             1\n",
      "629  2.944457  624.275140             1\n",
      "102  3.405693  673.922493             0\n",
      "270  3.040767  749.832685             0\n",
      "530  2.677698  671.372869             1\n",
      "315  3.511414  589.376252             0\n",
      "354  2.819195  415.405902             0\n",
      "349  3.202372  481.429935             0\n",
      "143  3.485599  623.116008             0\n",
      "61   3.049961  602.001883             0\n",
      "269  2.859058  485.142735             0\n",
      "620  3.412468  724.141783             1\n",
      "458  3.623912  430.964848             1\n",
      "115  3.337510  569.982835             0\n",
      "85   3.336941  489.931757             0\n",
      "48   3.205333  566.913704             0\n",
      "360  3.184187  683.380600             0\n",
      "136  3.136414  594.983239             0\n",
      "133  3.474318  661.969399             0\n",
      "361  3.042815  605.017109             0\n",
      "340  3.260019  662.107798             0\n",
      "451  3.636404  694.623595             1\n",
      "\n",
      "[644 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(8)\n",
    "shuffled_index = np.random.permutation(admissions.index)\n",
    "shuffled_admissions = admissions.loc[shuffled_index]\n",
    "\n",
    "train = shuffled_admissions.iloc[0:515]\n",
    "\n",
    "test = shuffled_admissions.iloc[515:len(shuffled_index)]\n",
    "\n",
    "print(shuffled_admissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Accuracy\n",
    "Instructions Train a logistic regression model using the gpa column from the train Dataframe.\n",
    "Use the LogisticRegression method predict to return the predicted labels for the gpa column from the test Dataframe. Assign the resulting list of labels to the predicted_label column in the test Dataframe.\n",
    "Calculate the accuracy of the predictions by dividing the number of rows where actual_label matches predicted_label by the total number of rows in the test set.\n",
    "Assign the accuracy value to accuracy and display it using the print function.\n",
    "Hint\n",
    "test[\"predicted_label\"] == test[\"actual_label\"] returns the index values for the rows where predicted_label matches actual_label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(train[[\"gpa\"]], train[\"actual_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_label = logistic_model.predict(test[[\"gpa\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>actual_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>3.219669</td>\n",
       "      <td>483.761856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2.799225</td>\n",
       "      <td>654.216375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>3.247795</td>\n",
       "      <td>518.556697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>3.567595</td>\n",
       "      <td>591.399738</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>3.448206</td>\n",
       "      <td>546.646023</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gpa         gre  actual_label  predicted_label\n",
       "117  3.219669  483.761856             0                0\n",
       "121  2.799225  654.216375             0                0\n",
       "214  3.247795  518.556697             0                0\n",
       "452  3.567595  591.399738             1                0\n",
       "447  3.448206  546.646023             1                0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"predicted_label\"] = predicted_label\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    122\n",
       "1      7\n",
       "Name: predicted_label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"predicted_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117    True\n",
       "121    True\n",
       "214    True\n",
       "324    True\n",
       "252    True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = test[\"predicted_label\"] == test[\"actual_label\"]\n",
    "correct_predictions = match[test[\"predicted_label\"] == test[\"actual_label\"]]\n",
    "correct_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpa                129\n",
       "gre                129\n",
       "actual_label       129\n",
       "predicted_label    129\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6356589147286822\n"
     ]
    }
   ],
   "source": [
    "accuracy = 82 / 129\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Sensitivity And Specificity\n",
    "Instructions Calculate the sensitivity value for the predictions on the test set and assign to sensitivity.\n",
    "Calculate the specificity value for the predictions on the test set and assign to specificity.\n",
    "Display both values using the print function.\n",
    "Hint\n",
    "Sensitivity = (# of True Positives) / (# of True Positives + # of False Negatives)\n",
    "Specificity = (# of True Negatives) / (# of False Positives + # of True Negatives)\n",
    "True Positive : True refers to predicted_label matching actual_label while Positive refers to actual_label being True (or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6356589147286822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train[[\"gpa\"]], train[\"actual_label\"])\n",
    "labels = model.predict(test[[\"gpa\"]])\n",
    "test[\"predicted_label\"] = labels\n",
    "matches = test[\"predicted_label\"] == test[\"actual_label\"]\n",
    "correct_predictions = test[matches]\n",
    "accuracy = len(correct_predictions) / len(test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "true_positive_filter = (test[\"predicted_label\"] == 1) & (test[\"actual_label\"] == 1)\n",
    "true_positives = len(test[true_positive_filter])\n",
    "false_negative_filter = (test[\"predicted_label\"] == 0) & (test[\"actual_label\"] == 1)\n",
    "false_negatives = len(test[false_negative_filter])\n",
    "\n",
    "sensitivity = true_positives / (true_positives + false_negatives)\n",
    "print(sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "false_positive_filter = (test[\"predicted_label\"] == 1) & (test[\"actual_label\"] == 0)\n",
    "false_positives = len(test[false_positive_filter])\n",
    "true_negative_filter = (test[\"predicted_label\"] == 0) & (test[\"actual_label\"] == 0)\n",
    "true_negatives = len(test[true_negative_filter])\n",
    "\n",
    "specificity = (true_negatives) / (false_positives + true_negatives)\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ROC Curve\n",
    "Instructions Import the relevant scikit-learn package you need to calculate the ROC curve.\n",
    "Use the model to return predicted probabilities for the test set.\n",
    "Use the roc_curve function to return the FPR and TPR values for different thresholds.\n",
    "Create and display a line plot with:\n",
    "the FPR values on the x-axis and\n",
    "the TPR values on the y-axis.\n",
    "Hint\n",
    "Documentation on predict_proba. Documentation on roc_curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "probabilities = model.predict_proba(test[[\"gpa\"]])\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test[\"actual_label\"], probabilities[:,1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Area Under The Curve\n",
    "Instructions Calculate the AUC score for our model on the training set and assign to auc_score.\n",
    "Use the print function to display auc_score.\n",
    "Hint\n",
    "Documentation on roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.577932098765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_score = metrics.roc_auc_score(test[\"actual_label\"], probabilities[:,1])\n",
    "print(auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
